{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61449e58-ad43-40e0-8e09-65cae2cbd074",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, torch, numpy as np\n",
    "import matplotlib as mpl, matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "from torch import tensor ,nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import coremltools as ct\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fed6d61a-2973-46bb-b076-ba25e37b20ce",
   "metadata": {},
   "source": [
    "### Configure GPU acceleration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6fa28d33-90a9-43e0-b039-b58887ea459f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mps\n"
     ]
    }
   ],
   "source": [
    "# cuda: NVIDIA GPUs, mps: Metal Performance Shaders (Apple M GPUs), cpu: generic CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37862f4d-ed65-4196-8591-92ae3d028d0b",
   "metadata": {},
   "source": [
    "### Import MNIST data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69999be2-7021-4f2c-aa6b-ea4d1c9662e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a transform to normalize the data\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Download and load the training data\n",
    "trainset = datasets.MNIST('./MNIST_data/', download=True, train=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=50000, shuffle=True)\n",
    "\n",
    "# Download and load the test data\n",
    "testset = datasets.MNIST('./MNIST_data/', download=True, train=False, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a294a219-9d80-4234-9ee4-4d48e69146b6",
   "metadata": {},
   "source": [
    "#### Create tensors for training and testing from data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8a66505-4fb9-40da-a421-70a58e372fad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, y_train = next(iter(trainloader))\n",
    "x_test, y_test = next(iter(testloader))\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3976511e-e81b-43f8-bfca-06e28779ccb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50000, 784])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = x_train.view(50000,-1)\n",
    "x_test = x_test.view(10000,-1)\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a6e12a5-9e51-4914-80a4-a9c9f7850622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_train[500])\n",
    "xsq = x_train.reshape((-1,28,28)).to(torch.device(\"cpu\"))\n",
    "_ = plt.imshow(xsq[500].view(28,28), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31df1451-c1bc-46ab-be55-5a0674ddf314",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3aff1b-a366-48eb-9450-2f323a825150",
   "metadata": {},
   "source": [
    "### Configure network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b41d9a9-2293-49f6-9244-8527b5290468",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (sigmoid): ReLU()\n",
      "  (Dense1): Linear(in_features=784, out_features=800, bias=True)\n",
      "  (Dense2): Linear(in_features=800, out_features=10, bias=True)\n",
      "  (dropout1): Dropout(p=0.0025, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# construct the model\n",
    "nin = x_train.shape[1]\n",
    "nout = int(np.max(y_train.numpy())+1)\n",
    "\n",
    "# Create Net class\n",
    "# nin: dimension of input data\n",
    "# nout: number of outputs\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,nin,nout):\n",
    "        super(Net,self).__init__()\n",
    "        self.sigmoid = nn.ReLU()\n",
    "        self.Dense1 = nn.Linear(nin,800)\n",
    "        self.Dense2 = nn.Linear(800, nout)\n",
    "        self.dropout1 = nn.Dropout(p=0.0025)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        # flatten image input\n",
    "        x = x.view(x.size(0), -1)\n",
    "        # layer 1\n",
    "        x = self.sigmoid(self.Dense1(x))\n",
    "        x = self.dropout1(x)\n",
    "        # layer 2\n",
    "        out = self.Dense2(x)\n",
    "        return out\n",
    "\n",
    "# Initialize network\n",
    "model = Net(nin=nin, nout=nout)\n",
    "\n",
    "# Print string representation\n",
    "print(str(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e443a66-c454-44e8-baba-7575eaa288f8",
   "metadata": {},
   "source": [
    "### Prepare dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9ebb10e6-8ad4-4a00-964b-59fd39e46773",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.view(50000,-1).to(device)\n",
    "y_train = y_train.to(device)\n",
    "\n",
    "train_ds = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "\n",
    "# Create a training/test data loader from datasets\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=100, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd543b1-15bf-4497-b067-306f030c1bd6",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "224d12bb-e425-44c5-967e-29e1a23272f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch:  1   Train Loss: 0.339   Train Acc: 89.78   \n",
      "========\n",
      "Epoch: 10   Train Loss: 0.038   Train Acc: 98.71   \n",
      "=========\n",
      "Epoch: 20   Train Loss: 0.019   Train Acc: 99.34   \n",
      "=========\n",
      "Epoch: 30   Train Loss: 0.015   Train Acc: 99.48   \n",
      "=========\n",
      "Epoch: 40   Train Loss: 0.015   Train Acc: 99.47   \n",
      "=========\n",
      "Epoch: 50   Train Loss: 0.007   Train Acc: 99.79   \n",
      "=========\n",
      "Epoch: 60   Train Loss: 0.013   Train Acc: 99.57   \n",
      "=========\n",
      "Epoch: 70   Train Loss: 0.006   Train Acc: 99.81   \n",
      "=========\n",
      "Epoch: 80   Train Loss: 0.011   Train Acc: 99.70   \n",
      "=========\n",
      "Epoch: 90   Train Loss: 0.004   Train Acc: 99.88   \n",
      "=========\n",
      "Epoch: 100   Train Loss: 0.005   Train Acc: 99.84   \n",
      "CPU times: user 6min 33s, sys: 36.4 s, total: 7min 9s\n",
      "Wall time: 8min 59s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "lr = 1e-3\n",
    "opt = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "a_tr_loss = []\n",
    "a_tr_accuracy = []\n",
    "\n",
    "num_epoch = 100\n",
    "\n",
    "for epoch in range(num_epoch):\n",
    "    model.train() # put model in training mode\n",
    "    correct = 0 # initialize error counter\n",
    "    total = 0 # initialize total counter\n",
    "    batch_loss_tr = []\n",
    "    # iterate over training set\n",
    "    for train_iter, data in enumerate(train_loader):\n",
    "        x_batch, y_batch = data\n",
    "        y_batch = y_batch.type(torch.long)\n",
    "        out = model(x_batch)\n",
    "        # Compute Loss\n",
    "        loss = criterion(out, y_batch)\n",
    "        batch_loss_tr.append(loss.item())\n",
    "        # Compute gradients using back propagation\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        # Take an optimization 'step'\n",
    "        opt.step()\n",
    "        # Do hard classification: index of largest score\n",
    "        _, predicted = torch.max(out.data, 1)\n",
    "        # Compute number of decision errors\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "        \n",
    "    a_tr_loss.append(np.mean(batch_loss_tr)) # Compute average loss over epoch\n",
    "    a_tr_accuracy.append(100*correct/total)\n",
    "\n",
    "    if ((epoch+1) % 10 == 0) or (epoch == 0):\n",
    "        print('\\nEpoch: {0:2d}   Train Loss: {1:.3f}   '.format(epoch+1, a_tr_loss[epoch])\n",
    "          +'Train Acc: {0:.2f}   '.format(a_tr_accuracy[epoch]))\n",
    "    elif ((epoch+1) % 1 == 0):\n",
    "        print('=', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3756ff0-4af9-4c08-8d98-88ec2f6dd79a",
   "metadata": {},
   "source": [
    "#### Evaluate model accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "db60fc77-89d7-4033-ab89-6032a98485ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy:  0.9792\n"
     ]
    }
   ],
   "source": [
    "# compute the training accuracy\n",
    "with torch.no_grad():\n",
    "    predict = model(x_test.to(device)).cpu().detach().numpy().argmax(axis=1)\n",
    "acc = np.mean(predict==y_test.numpy())\n",
    "print('training accuracy: ',acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6887fa16-a6db-4bf9-a797-e13d4b7e90ca",
   "metadata": {},
   "source": [
    "## Export as Core ML\n",
    "In order to use this model in an Xcode app, it must be converted to Core ML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74086b41-57ec-4f51-8b32-560b3b290255",
   "metadata": {},
   "source": [
    "#### Wrap trained model with softmax on output\n",
    "After the model is trained, we want to add softmax to the output.\n",
    "This is so that the CoreML model can output probabilities which can be used later by an app, for example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2af30e4-3b57-4469-b0a9-3b168a9e90af",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super(SoftmaxWrapper, self).__init__()\n",
    "        self.model = model\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        logits = self.model(input)\n",
    "        probabilities = self.softmax(logits)\n",
    "        return probabilities\n",
    "\n",
    "# wrap trained model\n",
    "model_with_softmax = SoftmaxWrapper(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541365bd-b589-436d-95ad-6f32f91b5709",
   "metadata": {},
   "source": [
    "### Convert to Core ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a0541ca9-3cfd-487c-9c0e-553c1d2af102",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting PyTorch Frontend ==> MIL Ops:  93%|███████████████████████████████████▎  | 13/14 [00:00<00:00, 3325.77 ops/s]\n",
      "Running MIL frontend_pytorch pipeline: 100%|██████████████████████████████████████| 5/5 [00:00<00:00, 13298.36 passes/s]\n",
      "Running MIL default pipeline:   0%|                                                         | 0/71 [00:00<?, ? passes/s]/opt/homebrew/Caskroom/miniconda/base/lib/python3.11/site-packages/coremltools/converters/mil/mil/passes/defs/preprocess.py:267: UserWarning: Output, '22', of the source model, has been renamed to 'var_22' in the Core ML model.\n",
      "  warnings.warn(msg.format(var.name, new_name))\n",
      "Running MIL default pipeline: 100%|██████████████████████████████████████████████| 71/71 [00:00<00:00, 3167.40 passes/s]\n",
      "Running MIL backend_mlprogram pipeline: 100%|███████████████████████████████████| 12/12 [00:00<00:00, 14904.25 passes/s]\n"
     ]
    }
   ],
   "source": [
    "model_with_softmax.eval()\n",
    "\n",
    "# create an example input tensor\n",
    "example_input = torch.rand(1, 1, 28, 28)\n",
    "\n",
    "# convert model to TorchScript via tracing\n",
    "traced_model = torch.jit.trace(model_with_softmax.cpu(), example_input)\n",
    "\n",
    "# convert TorchScript model to CoreML\n",
    "coreml_model = ct.convert(\n",
    "    traced_model,\n",
    "    convert_to=\"mlprogram\",\n",
    "    inputs=[ct.ImageType(name=\"Input\", \n",
    "                         shape=(1, 1, 28, 28), \n",
    "                         color_layout=ct.colorlayout.GRAYSCALE, \n",
    "                         bias=0.5\n",
    "                        )],\n",
    "    classifier_config = ct.ClassifierConfig(class_labels=list(range(10)), predicted_feature_name=\"Prediction\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6393de75-e0f7-4799-9b3c-ab2c0f4c3de2",
   "metadata": {},
   "source": [
    "#### Add metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fee2a16d-35d0-4ddf-8627-c09e617523b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "coreml_model.input_description[\"Input\"] = \"Input image containing a white number on a black background\"\n",
    "coreml_model.output_description[\"Prediction\"] = \"Predicted number\"\n",
    "\n",
    "coreml_model.author = \"Dhruv Weaver\"\n",
    "\n",
    "# coreml_model.license = \"\"\n",
    "\n",
    "coreml_model.short_description = \"Classifies written numbers using a model trained on MNIST dataset\"\n",
    "\n",
    "# set the preview type\n",
    "coreml_model.user_defined_metadata[\"com.apple.coreml.model.preview.type\"] = \"imageClassifier\"\n",
    "\n",
    "coreml_model.version = \"0.1\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decff053-e688-42f1-ba41-41bc456151ee",
   "metadata": {},
   "source": [
    "#### Test converted model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab88fe7e-176c-4211-8183-7aadeef24eb1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/wAALCAAcABwBAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APn+tjwzo51/xPpukDfi7uEiYp1VSRuPQ9Bk/hVrxroVr4Z8Y6lo1ncm5gtZAiytjd90Eg44yCSD9K52rFsLdrmIXTyRwFh5jRKGYL3IBIBP4ivV/Avifwn4Z8TaXbeGrS9uby/uoba41HVAq+TEzgMsaITgkdya434moY/iZ4hBxk3rng568/1rkqv6RZRajq9rZ3F5FZwzyqj3M33IgT94+1ehzx/DDwre2/2bUtd1fUrR1kF1ZCJIDIpBGN4PH/fQrD8Ya74V16XUNQsNN1RNVvbgTNLc3CGOPJJYBVA6+/TFcTRRRRX/2Q==",
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAABzklEQVR4Ab1SPYsUQRB93V0907OzX64cKKJyoHjJcelmJmIgotEFBycGiompgqmpmWaa+AMMjRQ0EAPhDEQuMVGQQwQPVryZc8fpmbJ6Z3Zh/QFWQ3dVv67H69cNtKFhLBwUARow8+3F6qAFihycQmexGxIr5yV6TqajYfonbGd44uHL6wjty5BKZ/Vb/oge4OwyGomc9BgffI8pjYOombLmjGGUyNf3u7vW5wWWO6Wy2j7l6kpQmjQdsulmt+pYmEfMfNkKp2gKoYKwWI4Q1M0JZ/UlJeCctSfihId0/9Y+v2HeJIEahyg5AEYTsz4+e2Ht193nuxx5sIwmrBbZF3cqzp+ch+PJtXAFWphru0J7bnUVSDdqfwdmjhCZoizJ1p9MQj7/sndyRFS2pNoXYrkvCtSZ7+Nwzxe+qFpzgk0MXUPHwBTRb/U+UUhRh2YdoWQidOspBhbVwBxOa+QNrxaKQJKpldMrZ8bDex9Gx9ceb6lWU7DIYHTqQcZZ9oP5z7fPfL9xgcCsuNq4fUOj+One7XSmR+jrq7ZRfkSssM2cP7sKbSG6Wq2yBv8J49cvtqAk7WOQhK8wlFwitYhIapJHlMcI327Q9M/g/z39BQNsgLu6sa8dAAAAAElFTkSuQmCC",
      "text/plain": [
       "<PIL.JpegImagePlugin.JpegImageFile image mode=L size=28x28>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 1\n",
      "Probabilites:\n",
      "{'Prediction': 1, 'var_22': array([[0., 1., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32), 'Prediction_probs': {0: 0.0, 9: 0.0, 5: 0.0, 1: 1.0, 6: 0.0, 2: 0.0, 7: 0.0, 3: 0.0, 8: 0.0, 4: 0.0}}\n"
     ]
    }
   ],
   "source": [
    "img = Image.open(\"image.jpg\")\n",
    "display(img)\n",
    "prediction = coreml_model.predict({\"Input\" : img})['Prediction']\n",
    "print(f'Prediction: {prediction}')\n",
    "print('Probabilites:')\n",
    "# print(coreml_model.predict({\"Input\" : img})['Prediction_probs'])\n",
    "print(coreml_model.predict({\"Input\" : img}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a51327-ee81-4b70-86be-3aa3650e9543",
   "metadata": {},
   "source": [
    "### Export model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a99a37f-17a4-4c7e-8455-35d997029df5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the CoreML model\n",
    "coreml_model.save('MNIST.mlpackage')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
